{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation: SQuAD + Trivia QA + Hotpot QA + Natural Questions QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "def process_squad(dataset: Any) -> List[Dict[str, str]]:\n",
    "    return [\n",
    "        {\n",
    "            \"context\": example['context'],\n",
    "            \"question\": example['question'],\n",
    "            \"answer\": example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "        }\n",
    "        for example in dataset['train']\n",
    "    ]\n",
    "\n",
    "def process_adversarial_qa(dataset: Any) -> List[Dict[str, str]]:\n",
    "    return [\n",
    "        {\n",
    "            \"context\": example['context'],\n",
    "            \"question\": example['question'],\n",
    "            \"answer\": example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "        }\n",
    "        for example in dataset['train']\n",
    "    ]\n",
    "\n",
    "def process_drop(dataset: Any) -> List[Dict[str, str]]:\n",
    "    return [\n",
    "        {\n",
    "            \"context\": example['passage'],\n",
    "            \"question\": example['question'],\n",
    "            \"answer\": example['answers_spans']['spans'][0] if example['answers_spans']['spans'] else \"\"\n",
    "        }\n",
    "        for example in dataset['train']\n",
    "    ]\n",
    "\n",
    "def process_duorc(dataset: Any) -> List[Dict[str, str]]:\n",
    "    return [\n",
    "        {\n",
    "            \"context\": example['plot'],\n",
    "            \"question\": example['question'],\n",
    "            \"answer\": example['answers'][0] if example['answers'] else \"\"\n",
    "        }\n",
    "        for example in dataset['train'] if not example['no_answer']\n",
    "    ]\n",
    "\n",
    "def process_coqa(dataset: Any) -> List[Dict[str, str]]:\n",
    "    return [\n",
    "        {\n",
    "            \"context\": example['story'],\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "        for example in dataset['train']\n",
    "        for question, answer in zip(example['questions'], example['answers']['input_text'])\n",
    "    ]\n",
    "\n",
    "def load_datasets():\n",
    "    print(\"Loading datasets...\")\n",
    "    datasets = {\n",
    "        \"SQuAD\": load_dataset(\"squad\"),\n",
    "        \"AdversarialQA\": load_dataset(\"adversarial_qa\", \"adversarialQA\"),\n",
    "        \"DROP\": load_dataset(\"ucinlp/drop\"),\n",
    "        \"DUO_RC\": load_dataset(\"ibm/duorc\", \"ParaphraseRC\"),\n",
    "        \"COQA\": load_dataset(\"stanfordnlp/coqa\")\n",
    "    }\n",
    "    print(\"Datasets loaded successfully.\")\n",
    "    return datasets\n",
    "\n",
    "def process_datasets(datasets):\n",
    "    merged_data = []\n",
    "\n",
    "    print(\"Processing SQuAD...\")\n",
    "    merged_data.extend(process_squad(datasets[\"SQuAD\"]))\n",
    "    print(\"Processing NewsQA...\")\n",
    "    merged_data.extend(process_adversarial_qa(datasets[\"AdversarialQA\"]))\n",
    "    print(\"Processing DROP...\")\n",
    "    merged_data.extend(process_drop(datasets[\"DROP\"]))\n",
    "    print(\"Processing DUO_RC...\")\n",
    "    merged_data.extend(process_duorc(datasets[\"DUO_RC\"]))\n",
    "    print(\"Processing COQA...\")\n",
    "    merged_data.extend(process_coqa(datasets[\"COQA\"]))\n",
    "\n",
    "    # Convert to DataFrame for easy handling\n",
    "    df = pd.DataFrame(merged_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "loaded_datasets = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_squad(dataset):\n",
    "    squad_df = pd.DataFrame(dataset['train'])\n",
    "    print(\"SQuAD Sample:\")\n",
    "    display(squad_df.head())\n",
    "\n",
    "def view_adversarial_qa(dataset):\n",
    "    adversarial_qa_df = pd.DataFrame(dataset['train'])\n",
    "    print(\"AdversarialQA Sample:\")\n",
    "    display(adversarial_qa_df.head())\n",
    "\n",
    "def view_drop(dataset):\n",
    "    drop_df = pd.DataFrame(dataset['train'])\n",
    "    print(\"DROP Sample:\")\n",
    "    display(drop_df.head())\n",
    "\n",
    "def view_duorc(dataset):\n",
    "    duorc_df = pd.DataFrame(dataset['train'])\n",
    "    print(\"DUO_RC Sample:\")\n",
    "    display(duorc_df.head())\n",
    "\n",
    "def view_coqa(dataset):\n",
    "    coqa_df = pd.DataFrame(dataset['train'])\n",
    "    print(\"COQA Sample:\")\n",
    "    display(coqa_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                     title  \\\n",
       "0  5733be284776f41900661182  University_of_Notre_Dame   \n",
       "1  5733be284776f4190066117f  University_of_Notre_Dame   \n",
       "2  5733be284776f41900661180  University_of_Notre_Dame   \n",
       "3  5733be284776f41900661181  University_of_Notre_Dame   \n",
       "4  5733be284776f4190066117e  University_of_Notre_Dame   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1  {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2  {'text': ['the Main Building'], 'answer_start'...  \n",
       "3  {'text': ['a Marian place of prayer and reflec...  \n",
       "4  {'text': ['a golden statue of the Virgin Mary'...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_squad(loaded_datasets['SQuAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdversarialQA Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7ba1e8f4261d3170fcf42e84a81dd749116fae95</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>What sare the benifts of the blood brain barrir?</td>\n",
       "      <td>{'text': ['isolated from the bloodstream'], 'a...</td>\n",
       "      <td>{'split': 'train', 'model_in_the_loop': 'Combi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ec5ef305a259311596e85d811ade30bd68b079d</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>What is surrounded by cerebrospinal fluid?</td>\n",
       "      <td>{'text': ['brain'], 'answer_start': [280]}</td>\n",
       "      <td>{'split': 'train', 'model_in_the_loop': 'Combi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7cb230edfb15ad1fda8d157af1f2b574cbb02b4c</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>What does the skull protect?</td>\n",
       "      <td>{'text': ['brain'], 'answer_start': [280]}</td>\n",
       "      <td>{'split': 'train', 'model_in_the_loop': 'Combi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e1850f2a48b8f7c2231cec41ed63c1b638a8e2c7</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>What has been injected into rats to produce pr...</td>\n",
       "      <td>{'text': ['chemicals'], 'answer_start': [723]}</td>\n",
       "      <td>{'split': 'train', 'model_in_the_loop': 'Combi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7bc0ae1a8a24ea4f3398b5236ab9569bbc3e820b</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>What can cause issues with how the brain works?</td>\n",
       "      <td>{'text': ['brain damage'], 'answer_start': [409]}</td>\n",
       "      <td>{'split': 'train', 'model_in_the_loop': 'Combi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  title  \\\n",
       "0  7ba1e8f4261d3170fcf42e84a81dd749116fae95  Brain   \n",
       "1  5ec5ef305a259311596e85d811ade30bd68b079d  Brain   \n",
       "2  7cb230edfb15ad1fda8d157af1f2b574cbb02b4c  Brain   \n",
       "3  e1850f2a48b8f7c2231cec41ed63c1b638a8e2c7  Brain   \n",
       "4  7bc0ae1a8a24ea4f3398b5236ab9569bbc3e820b  Brain   \n",
       "\n",
       "                                             context  \\\n",
       "0  Another approach to brain function is to exami...   \n",
       "1  Another approach to brain function is to exami...   \n",
       "2  Another approach to brain function is to exami...   \n",
       "3  Another approach to brain function is to exami...   \n",
       "4  Another approach to brain function is to exami...   \n",
       "\n",
       "                                            question  \\\n",
       "0   What sare the benifts of the blood brain barrir?   \n",
       "1         What is surrounded by cerebrospinal fluid?   \n",
       "2                       What does the skull protect?   \n",
       "3  What has been injected into rats to produce pr...   \n",
       "4    What can cause issues with how the brain works?   \n",
       "\n",
       "                                             answers  \\\n",
       "0  {'text': ['isolated from the bloodstream'], 'a...   \n",
       "1         {'text': ['brain'], 'answer_start': [280]}   \n",
       "2         {'text': ['brain'], 'answer_start': [280]}   \n",
       "3     {'text': ['chemicals'], 'answer_start': [723]}   \n",
       "4  {'text': ['brain damage'], 'answer_start': [409]}   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'split': 'train', 'model_in_the_loop': 'Combi...  \n",
       "1  {'split': 'train', 'model_in_the_loop': 'Combi...  \n",
       "2  {'split': 'train', 'model_in_the_loop': 'Combi...  \n",
       "3  {'split': 'train', 'model_in_the_loop': 'Combi...  \n",
       "4  {'split': 'train', 'model_in_the_loop': 'Combi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_adversarial_qa(loaded_datasets['AdversarialQA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_id</th>\n",
       "      <th>query_id</th>\n",
       "      <th>passage</th>\n",
       "      <th>question</th>\n",
       "      <th>answers_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nfl_2201</td>\n",
       "      <td>f16c0ee7-f131-4a8b-a6ac-4d275ea68066</td>\n",
       "      <td>To start the season, the Lions traveled south ...</td>\n",
       "      <td>How many points did the buccaneers need to tie...</td>\n",
       "      <td>{'spans': ['3'], 'types': ['number']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nfl_2201</td>\n",
       "      <td>c9582e03-b01b-42ed-83e0-b90a5334aefa</td>\n",
       "      <td>To start the season, the Lions traveled south ...</td>\n",
       "      <td>How many field goals did the Lions score?</td>\n",
       "      <td>{'spans': ['2'], 'types': ['number']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nfl_2201</td>\n",
       "      <td>f703d43d-73fa-4fda-8913-d81bd5569700</td>\n",
       "      <td>To start the season, the Lions traveled south ...</td>\n",
       "      <td>How long was the Lion's longest field goal?</td>\n",
       "      <td>{'spans': ['28-yard'], 'types': ['span']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nfl_2201</td>\n",
       "      <td>2fd4f473-af2b-44ce-929a-20c82fa6be2c</td>\n",
       "      <td>To start the season, the Lions traveled south ...</td>\n",
       "      <td>Who caught the touchdown for the fewest yard?</td>\n",
       "      <td>{'spans': ['Mike Williams'], 'types': ['span']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nfl_2201</td>\n",
       "      <td>6592e06d-4ad6-484f-a9a5-5cb72c76dfee</td>\n",
       "      <td>To start the season, the Lions traveled south ...</td>\n",
       "      <td>Who caught the shortest touchdown pass?</td>\n",
       "      <td>{'spans': ['Calvin Johnson'], 'types': ['span']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  section_id                              query_id  \\\n",
       "0   nfl_2201  f16c0ee7-f131-4a8b-a6ac-4d275ea68066   \n",
       "1   nfl_2201  c9582e03-b01b-42ed-83e0-b90a5334aefa   \n",
       "2   nfl_2201  f703d43d-73fa-4fda-8913-d81bd5569700   \n",
       "3   nfl_2201  2fd4f473-af2b-44ce-929a-20c82fa6be2c   \n",
       "4   nfl_2201  6592e06d-4ad6-484f-a9a5-5cb72c76dfee   \n",
       "\n",
       "                                             passage  \\\n",
       "0  To start the season, the Lions traveled south ...   \n",
       "1  To start the season, the Lions traveled south ...   \n",
       "2  To start the season, the Lions traveled south ...   \n",
       "3  To start the season, the Lions traveled south ...   \n",
       "4  To start the season, the Lions traveled south ...   \n",
       "\n",
       "                                            question  \\\n",
       "0  How many points did the buccaneers need to tie...   \n",
       "1          How many field goals did the Lions score?   \n",
       "2        How long was the Lion's longest field goal?   \n",
       "3      Who caught the touchdown for the fewest yard?   \n",
       "4            Who caught the shortest touchdown pass?   \n",
       "\n",
       "                                      answers_spans  \n",
       "0             {'spans': ['3'], 'types': ['number']}  \n",
       "1             {'spans': ['2'], 'types': ['number']}  \n",
       "2         {'spans': ['28-yard'], 'types': ['span']}  \n",
       "3   {'spans': ['Mike Williams'], 'types': ['span']}  \n",
       "4  {'spans': ['Calvin Johnson'], 'types': ['span']}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_drop(loaded_datasets['DROP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUO_RC Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plot_id</th>\n",
       "      <th>plot</th>\n",
       "      <th>title</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>no_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Set in the second half of the 22nd century, Ma...</td>\n",
       "      <td>Ghosts of Mars</td>\n",
       "      <td>28ded42d-f6d5-aac6-cf6f-9e6e0820c5aa</td>\n",
       "      <td>who is there with Melanie Ballard?</td>\n",
       "      <td>[second in command Sergeant  Jericho and priso...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Set in the second half of the 22nd century, Ma...</td>\n",
       "      <td>Ghosts of Mars</td>\n",
       "      <td>e7db917a-426b-62c1-01f8-a8eff0a71880</td>\n",
       "      <td>Who is colonized by a high tech company?</td>\n",
       "      <td>[Humans on Mars, Mars]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Set in the second half of the 22nd century, Ma...</td>\n",
       "      <td>Ghosts of Mars</td>\n",
       "      <td>43b629d6-29b2-473a-4f09-9302c17ddd24</td>\n",
       "      <td>Where is Melanie Ballard?</td>\n",
       "      <td>[in the hospital, in a remote mining town in m...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Set in the second half of the 22nd century, Ma...</td>\n",
       "      <td>Ghosts of Mars</td>\n",
       "      <td>d73065ee-964b-aae9-0420-a62c507b63ed</td>\n",
       "      <td>How did the police arrive at the Mars mining c...</td>\n",
       "      <td>[Space ship, Train]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Set in the second half of the 22nd century, Ma...</td>\n",
       "      <td>Ghosts of Mars</td>\n",
       "      <td>518eaeb8-05bd-5d29-db2f-1d86ea218034</td>\n",
       "      <td>What is the problem with the miners</td>\n",
       "      <td>[possessed, their body's were taken over by th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     plot_id                                               plot  \\\n",
       "0  /m/03vyhn  Set in the second half of the 22nd century, Ma...   \n",
       "1  /m/03vyhn  Set in the second half of the 22nd century, Ma...   \n",
       "2  /m/03vyhn  Set in the second half of the 22nd century, Ma...   \n",
       "3  /m/03vyhn  Set in the second half of the 22nd century, Ma...   \n",
       "4  /m/03vyhn  Set in the second half of the 22nd century, Ma...   \n",
       "\n",
       "            title                           question_id  \\\n",
       "0  Ghosts of Mars  28ded42d-f6d5-aac6-cf6f-9e6e0820c5aa   \n",
       "1  Ghosts of Mars  e7db917a-426b-62c1-01f8-a8eff0a71880   \n",
       "2  Ghosts of Mars  43b629d6-29b2-473a-4f09-9302c17ddd24   \n",
       "3  Ghosts of Mars  d73065ee-964b-aae9-0420-a62c507b63ed   \n",
       "4  Ghosts of Mars  518eaeb8-05bd-5d29-db2f-1d86ea218034   \n",
       "\n",
       "                                            question  \\\n",
       "0                 who is there with Melanie Ballard?   \n",
       "1           Who is colonized by a high tech company?   \n",
       "2                          Where is Melanie Ballard?   \n",
       "3  How did the police arrive at the Mars mining c...   \n",
       "4                What is the problem with the miners   \n",
       "\n",
       "                                             answers  no_answer  \n",
       "0  [second in command Sergeant  Jericho and priso...      False  \n",
       "1                             [Humans on Mars, Mars]      False  \n",
       "2  [in the hospital, in a remote mining town in m...      False  \n",
       "3                                [Space ship, Train]      False  \n",
       "4  [possessed, their body's were taken over by th...      False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_duorc(loaded_datasets['DUO_RC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COQA Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>story</th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>[When was the Vat formally opened?, what is th...</td>\n",
       "      <td>{'input_text': ['It was formally established i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnn</td>\n",
       "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
       "      <td>[Where was the Auction held?, How much did the...</td>\n",
       "      <td>{'input_text': ['Hard Rock Cafe', '$2 million....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gutenberg</td>\n",
       "      <td>CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...</td>\n",
       "      <td>[What did Venters call Lassiter?, Who asked La...</td>\n",
       "      <td>{'input_text': ['gun-man', 'Jane', 'Yes', 'to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn</td>\n",
       "      <td>(CNN) -- The longest-running holiday special s...</td>\n",
       "      <td>[Who is Rudolph's father?, Why does Rudolph ru...</td>\n",
       "      <td>{'input_text': ['Donner', 'he felt like an out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gutenberg</td>\n",
       "      <td>CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...</td>\n",
       "      <td>[Who arrived at the church?, Who was followed ...</td>\n",
       "      <td>{'input_text': ['the garrison first', 'Fra. Do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                              story  \\\n",
       "0  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
       "1        cnn  New York (CNN) -- More than 80 Michael Jackson...   \n",
       "2  gutenberg  CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...   \n",
       "3        cnn  (CNN) -- The longest-running holiday special s...   \n",
       "4  gutenberg  CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...   \n",
       "\n",
       "                                           questions  \\\n",
       "0  [When was the Vat formally opened?, what is th...   \n",
       "1  [Where was the Auction held?, How much did the...   \n",
       "2  [What did Venters call Lassiter?, Who asked La...   \n",
       "3  [Who is Rudolph's father?, Why does Rudolph ru...   \n",
       "4  [Who arrived at the church?, Who was followed ...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'input_text': ['It was formally established i...  \n",
       "1  {'input_text': ['Hard Rock Cafe', '$2 million....  \n",
       "2  {'input_text': ['gun-man', 'Jane', 'Yes', 'to ...  \n",
       "3  {'input_text': ['Donner', 'he felt like an out...  \n",
       "4  {'input_text': ['the garrison first', 'Fra. Do...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_coqa(loaded_datasets['COQA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SQuAD...\n",
      "Processing NewsQA...\n",
      "Processing DROP...\n",
      "Processing DUO_RC...\n",
      "Processing COQA...\n"
     ]
    }
   ],
   "source": [
    "# Process the datasets\n",
    "merged_dataset = process_datasets(loaded_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362393</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>Who was a sub?</td>\n",
       "      <td>Xabi Alonso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362394</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>Was it his first game this year?</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362395</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>What position did the team reach?</td>\n",
       "      <td>third</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362396</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>Who was ahead of them?</td>\n",
       "      <td>Barca.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362397</th>\n",
       "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
       "      <td>By how much?</td>\n",
       "      <td>six points</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362398 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  context  \\\n",
       "0       Architecturally, the school has a Catholic cha...   \n",
       "1       Architecturally, the school has a Catholic cha...   \n",
       "2       Architecturally, the school has a Catholic cha...   \n",
       "3       Architecturally, the school has a Catholic cha...   \n",
       "4       Architecturally, the school has a Catholic cha...   \n",
       "...                                                   ...   \n",
       "362393  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "362394  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "362395  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "362396  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "362397  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
       "\n",
       "                                                 question  \\\n",
       "0       To whom did the Virgin Mary allegedly appear i...   \n",
       "1       What is in front of the Notre Dame Main Building?   \n",
       "2       The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                       What is the Grotto at Notre Dame?   \n",
       "4       What sits on top of the Main Building at Notre...   \n",
       "...                                                   ...   \n",
       "362393                                     Who was a sub?   \n",
       "362394                   Was it his first game this year?   \n",
       "362395                  What position did the team reach?   \n",
       "362396                             Who was ahead of them?   \n",
       "362397                                       By how much?   \n",
       "\n",
       "                                         answer  \n",
       "0                    Saint Bernadette Soubirous  \n",
       "1                     a copper statue of Christ  \n",
       "2                             the Main Building  \n",
       "3       a Marian place of prayer and reflection  \n",
       "4            a golden statue of the Virgin Mary  \n",
       "...                                         ...  \n",
       "362393                              Xabi Alonso  \n",
       "362394                                      Yes  \n",
       "362395                                    third  \n",
       "362396                                   Barca.  \n",
       "362397                               six points  \n",
       "\n",
       "[362398 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the merged dataset\n",
    "merged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'merged_dataset' is now our combined dataset variable. For training, this dataset contains over 360k examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.text import CHRFScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        input_text = f\"context: {row['context']} answer: {row['answer']}\"\n",
    "        target_text = f\"question: {row['question']}\"\n",
    "\n",
    "        input_encoding = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        target_encoding = self.tokenizer(target_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_encoding[\"input_ids\"].squeeze(),\n",
    "            \"target_text\": target_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ce_loss(ce_loss):\n",
    "    return 1 - torch.exp(-ce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity(generated_questions, tokenizer):\n",
    "    # Tokenize and get embeddings\n",
    "    embeddings = tokenizer(generated_questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    embeddings = embeddings[\"input_ids\"].float().mean(dim=1)  # Simple mean embedding\n",
    "    \n",
    "    # Calculate pairwise cosine similarity\n",
    "    similarity_matrix = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=2)\n",
    "    \n",
    "    # Remove self-similarity from diagonal\n",
    "    similarity_matrix.fill_diagonal_(0)\n",
    "    \n",
    "    # Calculate mean similarity (lower is more diverse)\n",
    "    mean_similarity = similarity_matrix.mean()\n",
    "    \n",
    "    return mean_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(ce_loss, generated_questions, target_questions, tokenizer):\n",
    "    # Weights\n",
    "    w1, w2, w3 = 0.5, 0.3, 0.2\n",
    "    \n",
    "    # Normalize cross-entropy loss\n",
    "    normalized_ce = normalize_ce_loss(ce_loss)\n",
    "    \n",
    "    # Calculate chrF score\n",
    "    chrf = CHRFScore()\n",
    "    chrf_score = chrf(generated_questions, target_questions)\n",
    "    chrf_loss = 1 - chrf_score\n",
    "    \n",
    "    # Calculate diversity\n",
    "    diversity = calculate_diversity(generated_questions, tokenizer)\n",
    "    \n",
    "    # Combine losses\n",
    "    combined_loss = w1 * normalized_ce + w2 * chrf_loss + w3 * diversity\n",
    "    \n",
    "    return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, tokenizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        target_texts = batch[\"target_text\"]\n",
    "\n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=50)\n",
    "        generated_questions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        ce_loss = outputs.loss\n",
    "\n",
    "        loss = custom_loss(ce_loss, generated_questions, target_texts, tokenizer)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You must have this library to use the T5 model. If you don't have it, install it by running the following command and restart the kernel:\n",
    "# %pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your pandas DataFrame\n",
    "df = merged_dataset\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "max_length = 512\n",
    "dataset = QuestionGenerationDataset(df, tokenizer, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/181199 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 4\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, tokenizer, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m target_texts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m generated_questions \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n",
      "File \u001b[1;32mc:\\Users\\serha\\miniconda3\\envs\\qgen\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\serha\\miniconda3\\envs\\qgen\\Lib\\site-packages\\transformers\\generation\\utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1755\u001b[0m     )\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1773\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\serha\\miniconda3\\envs\\qgen\\Lib\\site-packages\\transformers\\generation\\utils.py:2392\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   2389\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2390\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[1;32m-> 2392\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m   2394\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2396\u001b[0m     \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train(model, dataloader, optimizer, tokenizer, device)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./t5_question_generator_advanced\")\n",
    "tokenizer.save_pretrained(\"./t5_question_generator_advanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If model is already loaded: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_inline(context, answer, model, tokenizer, device):\n",
    "    # Prepare the input\n",
    "    input_text = f\"context: {context} answer: {answer}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(device)\n",
    "\n",
    "    # Generate the question\n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_length=64, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated question\n",
    "    generated_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove the \"question: \" prefix if it's present\n",
    "    if generated_question.lower().startswith(\"question:\"):\n",
    "        generated_question = generated_question[len(\"question:\"):].strip()\n",
    "\n",
    "    return generated_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model, tokenizer, and device are already defined\n",
    "context = \"The Great Wall of China is an ancient wall in China. The wall is thousands of miles long and took many years to build.\"\n",
    "answer = \"The Great Wall of China\"\n",
    "\n",
    "question = generate_question_inline(context, answer, model, tokenizer, device)\n",
    "print(f\"Generated Question: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If model is not loaded (Cold Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def generate_question(context, answer, model_path=\"./t5_question_generator_advanced\"):\n",
    "    # Set up device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "    # Prepare the input\n",
    "    input_text = f\"context: {context} answer: {answer}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(device)\n",
    "\n",
    "    # Generate the question\n",
    "    outputs = model.generate(input_ids, max_length=64, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)\n",
    "\n",
    "    # Decode and return the generated question\n",
    "    generated_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove the \"question: \" prefix if it's present\n",
    "    if generated_question.lower().startswith(\"question:\"):\n",
    "        generated_question = generated_question[len(\"question:\"):].strip()\n",
    "\n",
    "    return generated_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The Great Wall of China is an ancient wall in China. The wall is thousands of miles long and took many years to build.\"\n",
    "answer = \"The Great Wall of China\"\n",
    "\n",
    "question = generate_question(context, answer)\n",
    "print(f\"Generated Question: {question}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
