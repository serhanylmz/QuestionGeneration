{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Some trials, samples with readymade QG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Who was the third human spaceflight program in the United States?\n",
      "Question 2: Who was the third human spaceflight program of the United States?\n",
      "Question 3: Who carried out the Apollo program?\n",
      "Question 4: Who carried out the third human spaceflight program?\n",
      "Question 5: Who carried out the third human spaceflight program in the United States?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def generate_questions(context, answer, num_questions=5):\n",
    "    model_name = \"valhalla/t5-small-qg-prepend\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    input_text = f\"answer: {answer} context: {context}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate a set of questions\n",
    "    questions = set()\n",
    "    while len(questions) < num_questions:\n",
    "        outputs = model.generate(input_ids, num_return_sequences=5, max_length=64, num_beams=5)\n",
    "        for output in outputs:\n",
    "            question = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            questions.add(question)\n",
    "\n",
    "    return list(questions)\n",
    "\n",
    "# Example usage\n",
    "context = \"The Apollo program was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which succeeded in landing the first humans on the Moon from 1969 to 1972.\"\n",
    "answer = \"National Aeronautics and Space Administration\"\n",
    "questions = generate_questions(context, answer)\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"Question {i+1}: {question}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Dataset from SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset by downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SQuAD dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c542f13c4d46cabdf44ff9aaaaaa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9b99aa13364e2e8a3c20bdf18f3163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1823cb66d28345ecba42ac2b37e4d52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af40814325a425bb6dbae6987813857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86facd44545b497bb96b7d3ef3fe4f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  87599\n",
      "validation:  10570\n",
      "Loading df_train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87599it [04:38, 314.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10570it [00:08, 1204.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling DataFrame...\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_squad_dataset(dataset):\n",
    "    df_dataset = pd.DataFrame(columns=['context', 'question', 'answer'])\n",
    "    num_of_answer = 0\n",
    "    for index, value in tqdm(enumerate(dataset)):\n",
    "        context = value['context']\n",
    "        question = value['question']\n",
    "        answer = value['answers']['text'][0]\n",
    "        number_of_words = len(answer.split())\n",
    "        df_dataset.loc[num_of_answer] = [context] + [question] + [answer]\n",
    "        num_of_answer = num_of_answer + 1\n",
    "    return df_dataset\n",
    "\n",
    "\n",
    "print('Downloading SQuAD dataset...')\n",
    "train_dataset = load_dataset(\"squad\", split='train')\n",
    "valid_dataset = load_dataset(\"squad\", split='validation')\n",
    "print('train: ', len(train_dataset))\n",
    "print('validation: ', len(valid_dataset))\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print('Loading df_train...')\n",
    "df_train = load_squad_dataset(train_dataset)\n",
    "print('Loading df_validation...')\n",
    "df_validation = load_squad_dataset(valid_dataset)\n",
    "\n",
    "print('Shuffling DataFrame...')\n",
    "df_train = shuffle(df_train)\n",
    "df_validation = shuffle(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See some examples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape\n",
      "(87599, 3)\n",
      "df_validation.shape\n",
      "(10570, 3)\n",
      "df_train.head():\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     context  \\\n",
      "4875                                                                                                                                                                                                                                                                                                                                                    The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 3,850,000 exajoules (EJ) per year. In 2002, this was more energy in one hour than the world used in one year. Photosynthesis captures approximately 3,000 EJ per year in biomass. The amount of solar energy reaching the surface of the planet is so vast that in one year it is about twice as much as will ever be obtained from all of the Earth's non-renewable resources of coal, oil, natural gas, and mined uranium combined,   \n",
      "42637  The company, which benefited from the imperial patronage, soon expanded its commercial trading operations, eclipsing the Portuguese Estado da Índia, which had established bases in Goa, Chittagong, and Bombay, which Portugal later ceded to England as part of the dowry of Catherine de Braganza. The East India Company also launched a joint attack with the Dutch United East India Company on Portuguese and Spanish ships off the coast of China, which helped secure their ports in China. The company established trading posts in Surat (1619), Madras (1639), Bombay (1668), and Calcutta (1690). By 1647, the company had 23 factories, each under the command of a factor or master merchant and governor if so chosen, and 90 employees in India. The major factories became the walled forts of Fort William in Bengal, Fort St George in Madras, and Bombay Castle.   \n",
      "36762                                                                                                                                                                                                                                        In 1971, Akira Endo, a Japanese biochemist working for the pharmaceutical company Sankyo, identified mevastatin (ML-236B), a molecule produced by the fungus Penicillium citrinum, as an inhibitor of HMG-CoA reductase, a critical enzyme used by the body to produce cholesterol. Animal trials showed very good inhibitory effect as in clinical trials, however a long term study in dogs found toxic effects at higher doses and as a result mevastatin was believed to be too toxic for human use. Mevastatin was never marketed, because of its adverse effects of tumors, muscle deterioration, and sometimes death in laboratory dogs.   \n",
      "20050                                                                                                                                                                                                                                               Subsequently, seven other chiefs on seven other islands signed a treaty in German and Marshallese and a final copy witnessed by Rötger on November 1 was sent to the German Foreign Office. The Germans erected a sign declaring a \"Imperial German Protectorate\" at Jaluit. It has been speculated that the crisis over the Carolines with Spain, which almost provoked a war, was in fact \"a feint to cover the acquisition of the Marshall Islands\", which went almost unnoticed at the time, despite the islands being the largest source of copra in Micronesia. Spain sold the islands to Germany in 1884 through papal mediation.   \n",
      "72564                                                                                                                                                                                                                                                                                                                      Red is the color at the end of the spectrum of visible light next to orange and opposite violet. Red color has a predominant light wavelength of roughly 620–740 nanometres. Red is one of the additive primary colors of visible light, along with green and blue, which in Red Green Blue (RGB) color systems are combined to create all the colors on a computer monitor or television screen. Red is also one of the subtractive primary colors, along with yellow and blue, of the RYB color space and traditional color wheel used by painters and artists.   \n",
      "\n",
      "                                                        question  \\\n",
      "4875   What is the amount of solar energy absorbed by the earth?   \n",
      "42637              when was the bombay trading post established?   \n",
      "36762                           Where did Endo discover ML-236B?   \n",
      "20050     In what year did Germany buy the Marshalls from Spain?   \n",
      "72564    Which color is opposite of red on the visible spectrum?   \n",
      "\n",
      "                                                answer  \n",
      "4875   approximately 3,850,000 exajoules (EJ) per year  \n",
      "42637                                             1668  \n",
      "36762                             Penicillium citrinum  \n",
      "20050                                             1884  \n",
      "72564                                           violet  \n",
      "df_validation.head():\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 context  \\\n",
      "5343  Major roads in the area include the A1 (Gateshead Newcastle Western Bypass), stretching north to Edinburgh and south to London; the A19 heading south past Sunderland and Middlesbrough to York and Doncaster; the A69 heading west to Carlisle; the A696, which becomes the A68 heads past Newcastle Airport and up through central Northumberland and central Scottish Borders, the A167, the old \"Great North Road\", heading south to Gateshead, Chester-le-Street, Durham and Darlington; and the A1058 \"Coast Road\", which runs from Jesmond to the east coast between Tynemouth and Cullercoats. Many of these designations are recent—upon completion of the Western Bypass, and its designation as the new line of the A1, the roads between this and the A1's former alignment through the Tyne Tunnel were renumbered, with many city centre roads changing from a 6-prefix to their present 1-prefix numbers. In November 2011 the capacity of the Tyne Tunnel was increased when a project to build a second road tunnel and refurbish the first tunnel was completed.   \n",
      "7922                                                                                                                                                                                                                                                                                                                                      In 2013, the Peabody Awards honoured Doctor Who with an Institutional Peabody \"for evolving with technology and the times like nothing else in the known television universe.\" The programme is listed in Guinness World Records as the longest-running science fiction television show in the world, the \"most successful\" science fiction series of all time—based on its over-all broadcast ratings, DVD and book sales, and iTunes traffic— and for the largest ever simulcast of a TV drama with its 50th anniversary special. During its original run, it was recognised for its imaginative stories, creative low-budget special effects, and pioneering use of electronic music (originally produced by the BBC Radiophonic Workshop).   \n",
      "7212                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League. Harvard has an intense athletic rivalry with Yale University culminating in The Game, although the Harvard–Yale Regatta predates the football game. This rivalry, though, is put aside every two years when the Harvard and Yale Track and Field teams come together to compete against a combined Oxford University and Cambridge University team, a competition that is the oldest continuous international amateur competition in the world.   \n",
      "657                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            In honor of the 50th Super Bowl, the pregame ceremony featured the on-field introduction of 39 of the 43 previous Super Bowl Most Valuable Players. Bart Starr (MVP of Super Bowls I and II) and Chuck Howley (MVP of Super Bowl V) appeared via video, while Peyton Manning (MVP of Super Bowl XLI and current Broncos quarterback) was shown in the locker room preparing for the game. No plans were announced regarding the recognition of Harvey Martin, co-MVP of Super Bowl XII, who died in 2001.   \n",
      "5525                                                                                                                                                                                      The smaller galleries cover Korea, the Himalayan kingdoms and South East Asia. Korean displays include green-glazed ceramics, silk embroideries from officials' robes and gleaming boxes inlaid with mother-of-pearl made between 500 AD and 2000. Himalayan items include important early Nepalese bronze sculptures, repoussé work and embroidery. Tibetan art from the 14th to the 19th century is represented by notable 14th- and 15th-century religious images in wood and bronze, scroll paintings and ritual objects. Art from Thailand, Burma, Cambodia, Indonesia and Sri Lanka in gold, silver, bronze, stone, terracotta and ivory represents these rich and complex cultures, the displays span the 6th to 19th centuries. Refined Hindu and Buddhist sculptures reflect the influence of India; items on show include betel-nut cutters, ivory combs and bronze palanquin hooks.   \n",
      "\n",
      "                                                                                           question  \\\n",
      "5343                                                      What's the nickname for Newcastle's A167?   \n",
      "7922  Where is Doctor Who the record holder for most successful science fiction series of all time?   \n",
      "7212                                            At what time is the Harvard-Yale rivalry set aside?   \n",
      "657                                                                         Which MVP died in 2001?   \n",
      "5525                          Which South Asian island nation is represented in the V&A collection?   \n",
      "\n",
      "                                                                                                                                                           answer  \n",
      "5343                                                                                                                                   the old \"Great North Road\"  \n",
      "7922                                                                                                                                       Guinness World Records  \n",
      "7212  every two years when the Harvard and Yale Track and Field teams come together to compete against a combined Oxford University and Cambridge University team  \n",
      "657                                                                                                                                                 Harvey Martin  \n",
      "5525                                                                                                                                                    Sri Lanka  \n"
     ]
    }
   ],
   "source": [
    "print('df_train.shape')\n",
    "print(df_train.shape)\n",
    "print('df_validation.shape')\n",
    "print(df_validation.shape)\n",
    "print('df_train.head():')\n",
    "print(df_train.head())\n",
    "print('df_validation.head():')\n",
    "print(df_validation.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset as csv...\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "print('Saving dataset as csv...')\n",
    "dataset_save_path = 'datasets/'\n",
    "if not os.path.exists(dataset_save_path):\n",
    "    os.makedirs(dataset_save_path)\n",
    "df_train.to_csv(dataset_save_path + 'squad_train.csv', index=False)\n",
    "df_validation.to_csv(dataset_save_path + 'squad_validation.csv', index=False)\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a QG-Algorithm, Model: T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Initial Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "train_file_path = 'datasets/squad_train.csv'\n",
    "validation_file_path = 'datasets/squad_validation.csv'\n",
    "save_model_path = 'model/'\n",
    "save_tokenizer_path = 'tokenizer/'\n",
    "pretrained_model = 't5-large'\n",
    "\n",
    "# Set training arguments\n",
    "args = {\n",
    "    'num_workers': 2,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 3e-5,\n",
    "    'eps': 1e-8,\n",
    "    'weight_decay': 0.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, max_len_input=512, max_len_output=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.max_len_input = max_len_input\n",
    "        self.max_len_output = max_len_output\n",
    "        self.context_column = 'context'\n",
    "        self.answer_column = 'answer'\n",
    "        self.question_column = 'question'\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self._load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index]['input_ids'].squeeze()\n",
    "        target_ids = self.targets[index]['input_ids'].squeeze()\n",
    "        source_mask = self.inputs[index]['attention_mask'].squeeze()\n",
    "        target_mask = self.targets[index]['attention_mask'].squeeze()\n",
    "        labels = copy.deepcopy(target_ids)\n",
    "        labels[labels == 0] = -100\n",
    "        return {'source_ids': source_ids, 'source_mask': source_mask, 'target_ids': target_ids, 'target_mask': target_mask, 'labels': labels}\n",
    "\n",
    "    def _load_data(self):\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            context, answer, target = self.data.loc[idx, self.context_column], self.data.loc[idx, self.answer_column], self.data.loc[idx, self.question_column]\n",
    "            input_text = '<answer> %s <context> %s ' % (answer, context)\n",
    "            target = str(target)\n",
    "\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_text],\n",
    "                max_length=self.max_len_input,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target],\n",
    "                max_length=self.max_len_output,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer, args):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=self.args['batch_size'], num_workers=self.args['num_workers'])\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(validation_dataset, batch_size=self.args['batch_size'], num_workers=self.args['num_workers'])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.args['learning_rate'], eps=self.args['eps'])\n",
    "    \n",
    "    def save_model(self, save_model_path):\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(save_model_path):\n",
    "            os.makedirs(save_model_path)\n",
    "        # Save the model\n",
    "        self.model.save_pretrained(save_model_path)\n",
    "        \n",
    "    def save_tokenizer(self, save_tokenizer_path):\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(save_tokenizer_path):\n",
    "            os.makedirs(save_tokenizer_path)\n",
    "        # Save the tokenizer\n",
    "        self.tokenizer.save_pretrained(save_tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading pre-trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [01:29<00:00, 980.14it/s] \n",
      "100%|██████████| 10570/10570 [00:11<00:00, 912.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:  87599\n",
      "validation_dataset:  10570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pl.seed_everything(99)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "print('Loading pre-trained model...')\n",
    "model = T5ForConditionalGeneration.from_pretrained(pretrained_model).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_model, model_max_length=512)\n",
    "tokenizer.add_special_tokens(\n",
    "    {'additional_special_tokens': ['<answer>', '<context>']}\n",
    ")\n",
    "\n",
    "print('Preparing dataset...')\n",
    "train_dataset = QGDataset(tokenizer, train_file_path)\n",
    "validation_dataset = QGDataset(tokenizer, validation_file_path)\n",
    "\n",
    "print('train_dataset: ', len(train_dataset))\n",
    "print('validation_dataset: ', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cta/users/serhan.yilmaz/.local/lib/python3.10/site- ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# torch.set_float32_matmul_precision('high')  # or 'high'\n",
    "\n",
    "print('Initializing model...')\n",
    "model = T5FineTuner(model, tokenizer, args)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 737 M \n",
      "-----------------------------------------------------\n",
      "737 M     Trainable params\n",
      "0         Non-trainable params\n",
      "737 M     Total params\n",
      "2,950.672 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52afa01b85445fc878db3fce6da32d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "print('Fine tuning...')\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Total time: 10.985352538426717 hours\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "print('Saving model...')\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.makedirs(save_model_path)\n",
    "if not os.path.exists(save_tokenizer_path):\n",
    "    os.makedirs(save_tokenizer_path)\n",
    "model.model.save_pretrained(save_model_path)\n",
    "tokenizer.save_pretrained(save_tokenizer_path)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print('Total time: %s hours' % (end_time / 60 / 60))\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "trained_model = 'all-distilroberta-v1'\n",
    "# trained_model = 'all-roberta-large-v1'\n",
    "class SentenceEmbeddings:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(trained_model)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.embedder.encode(text, convert_to_tensor=True)\n",
    "\n",
    "    def get_most_similar(self, context:str, qa_list:list):\n",
    "        context_embeddings = self.encode(context)\n",
    "        top1 = {'idx': 0, 'score': float('-inf')}\n",
    "        for i in range(len(qa_list)):\n",
    "            qa_str = qa_list[i]['question'] + ' ' + qa_list[i]['answer']\n",
    "            qa_embeddings = self.encode(qa_str)\n",
    "            cos_score = util.pytorch_cos_sim(context_embeddings, qa_embeddings)\n",
    "            # print(cos_score[0][0], qa_list[i])\n",
    "            if cos_score[0][0] > top1['score']:\n",
    "                top1['score'] = cos_score[0][0]\n",
    "                top1['idx'] = i\n",
    "        return qa_list[top1['idx']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QG-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "trained_model_path = save_model_path\n",
    "trained_tokenizer_path = save_tokenizer_path\n",
    "\n",
    "class QuestionGeneration:\n",
    "\n",
    "    def __init__(self, model_dir=None):\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer_path)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate(self, answer: str, context: str):\n",
    "        input_text = '<answer> %s <context> %s ' % (answer, context)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams = 3,\n",
    "            num_return_sequences = 1\n",
    "        )\n",
    "        question_list = []\n",
    "        for output in outputs:\n",
    "            question = self.tokenizer.decode(\n",
    "                output,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            question_list.append({'question': question, 'answer': answer, 'context': context})\n",
    "        return question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who has fine-tuned T5 on SQuAD dataset for question generation?', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'On what dataset has Serhan fine-tuned T5?', 'answer': 'SQuAD', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'What did Serhan fine-tune the T5 on SQuAD dataset for?', 'answer': 'question generation', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n"
     ]
    }
   ],
   "source": [
    "context = '''\n",
    "Serhan has fine-tuned T5 on SQuAD dataset for question generation.\n",
    "'''\n",
    "answer_list = ['Serhan', 'SQuAD', 'question generation']\n",
    "\n",
    "QG = QuestionGeneration()\n",
    "SE = SentenceEmbeddings()\n",
    "\n",
    "for answer in answer_list:\n",
    "    qa_pair_list = QG.generate(answer, context)\n",
    "    most_similar = SE.get_most_similar(context, qa_pair_list)\n",
    "    print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/serhan-ubuntu/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who has fine-tuned T5 on SQuAD dataset for question generation?', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'On what dataset has Serhan fine-tuned T5?', 'answer': 'SQuAD', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'Serhan has fine-tuned T5 on SQuAD dataset for what?', 'answer': 'question generation', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'Who has fine-tuned T5 on SQuAD dataset for question generation?', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n"
     ]
    }
   ],
   "source": [
    "context = '''\n",
    "Serhan has fine-tuned T5 on SQuAD dataset for question generation.\n",
    "'''\n",
    "answer_list = ['Serhan', 'SQuAD', 'question generation', 'Serhan']\n",
    "\n",
    "QG = QuestionGeneration()\n",
    "SE = SentenceEmbeddings()\n",
    "\n",
    "for answer in answer_list:\n",
    "    qa_pair_list = QG.generate(answer, context)\n",
    "    most_similar = SE.get_most_similar(context, qa_pair_list)\n",
    "    print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
