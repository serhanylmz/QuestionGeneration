{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "680ef7a2-9827-4051-b014-3bf717298fbe",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf4909-76c1-461d-8de5-3612f21f93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from datasets import load_dataset\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4e0cb2-3d52-4539-b14d-5304fa751592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter_session\n"
     ]
    }
   ],
   "source": [
    "!tmux display-message -p '#S'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2f984-8a4b-4ed1-a334-e872651b237b",
   "metadata": {},
   "source": [
    "## Load and Process the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7a181-ae5c-442a-bc5f-949ff8d26aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"quora\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e9ab9-5a99-4da6-a5ca-02f973ce4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x['is_duplicate'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e97c8-0da3-446e-a94e-484798a773aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to flatten and prepare the data\n",
    "def prepare_data(examples):\n",
    "    # Create lists to store processed examples\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    # Process each entry\n",
    "    for question_pair in examples['questions']:\n",
    "        # Assuming each entry in 'questions' has two questions\n",
    "        if len(question_pair['text']) == 2:\n",
    "            input_texts.append(\"paraphrase: \" + question_pair['text'][0])\n",
    "            target_texts.append(question_pair['text'][1])\n",
    "    \n",
    "    # Return a dictionary of processed examples\n",
    "    return {'input_text': input_texts, 'target_text': target_texts}\n",
    "\n",
    "# Apply the function to each entry in the dataset\n",
    "processed_datasets = dataset.map(prepare_data, batched=True, remove_columns=['questions', 'is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5865ad-3edb-4e0a-ab7b-704a04c1fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a3eb7-64cb-42ef-ab59-77066b2c8faf",
   "metadata": {},
   "source": [
    "## Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6d211-5019-447c-a2c8-565acf04fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-3b')\n",
    "\n",
    "# Define the function to tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['input_text'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['target_text'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fb5ab-cf95-4a22-bf2f-5f1a116d79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to all sets in the dataset\n",
    "tokenized_datasets = processed_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b1629-a237-465e-97f0-1051e57cb318",
   "metadata": {},
   "source": [
    "## Prepare the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd68cc-c674-46ea-a253-06e99e28325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a helper function to create the DataLoader\n",
    "def create_dataloader(tokenized_data, batch_size=8):\n",
    "    # Convert list of dictionaries into a format DataLoader can handle\n",
    "    dataset = tokenized_data.remove_columns(['input_text', 'target_text'])  # Remove text columns not needed for training\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    # Create the DataLoader\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoaders for training (and optionally validation)\n",
    "train_dataloader = create_dataloader(tokenized_datasets['train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78147a7e-0834-4fbb-99be-7581378f2623",
   "metadata": {},
   "source": [
    "## Load Model / Set Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a63ca1b-eb8a-4271-b9cb-f87e82a8d1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cta/users/serhan.yilmaz/.local/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-3b').cuda()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 3\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2fd9c-6438-4a31-af12-56d494ff8a68",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9206b0-d3f9-4315-b9cc-d12afac6c5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be39fbdaf2fc424598653c501df1924f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6caa8cf-5233-4536-a213-e53aee9a466b",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2db20a4-be0c-42ff-bf49-ade31f145138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t5_paraphrase_model/tokenizer_config.json',\n",
       " './t5_paraphrase_model/special_tokens_map.json',\n",
       " './t5_paraphrase_model/spiece.model',\n",
       " './t5_paraphrase_model/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./t5_paraphrase_model\")\n",
    "tokenizer.save_pretrained(\"./t5_paraphrase_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86141478-9135-4f24-b5ae-60cfc2b35132",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01458f20-beaa-48f1-b294-1e1df0315808",
   "metadata": {},
   "source": [
    "## Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c41b9f-dfcc-4223-937a-75288d378498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = \"./t5_paraphrase_model\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).cuda()\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb9743-807c-4fa5-a129-fab99f8be871",
   "metadata": {},
   "source": [
    "## Function to Generate Paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7bad06d-fe04-4fac-92e9-1407142f390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paraphrases(input_text, num_returns=3):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(\"paraphrase: \" + input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate paraphrases\n",
    "    paraphrases = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        num_beams=num_returns,\n",
    "        num_return_sequences=num_returns,\n",
    "        no_repeat_ngram_size=1,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode and print each paraphrase\n",
    "    return [tokenizer.decode(paraphrase, skip_special_tokens=True) for paraphrase in paraphrases]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702a70e-a3e8-42ef-b42a-c4d52843b5eb",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a8a186a-30af-4563-9527-cae56b54eb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase 1: What is the best way to learn artificial intelligence?\n",
      "Paraphrase 2: How can I learn artificial intelligence?\n",
      "Paraphrase 3: How can I learn Artificial Intelligence?\n",
      "Paraphrase 4: How do I learn artificial intelligence?\n",
      "Paraphrase 5: How do I learn Artificial Intelligence?\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_sentence = \"What is the best way to learn artificial intelligence?\"\n",
    "paraphrase_outputs = generate_paraphrases(input_sentence, num_returns=5)\n",
    "for i, paraphrase in enumerate(paraphrase_outputs, 1):\n",
    "    print(f\"Paraphrase {i}: {paraphrase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f9f5261-43b6-4815-9f52-1818d2f36890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase 1: What occupation did Albert Einstein have?\n",
      "Paraphrase 2: What was the occupation of Albert Einstein?\n",
      "Paraphrase 3: What was Albert Einstein's occupation?\n",
      "Paraphrase 4: What occupation did Albert Einstein hold?\n",
      "Paraphrase 5: What occupation did Einstein have?\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_sentence = \"What occupation did Albert Einstein have?\"\n",
    "paraphrase_outputs = generate_paraphrases(input_sentence, num_returns=5)\n",
    "for i, paraphrase in enumerate(paraphrase_outputs, 1):\n",
    "    print(f\"Paraphrase {i}: {paraphrase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d8d89f7-aa15-4018-ab28-fbe41a03daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase 1: What nationality did Albert Einstein have?\n",
      "Paraphrase 2: What was the nationality of Albert Einstein?\n",
      "Paraphrase 3: What nationality was Albert Einstein?\n",
      "Paraphrase 4: What nationality did Albert Einstein hold?\n",
      "Paraphrase 5: Who was Albert Einstein?\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_sentence = \"What nationality did the physicist Albert Einstein have?\"\n",
    "paraphrase_outputs = generate_paraphrases(input_sentence, num_returns=5)\n",
    "for i, paraphrase in enumerate(paraphrase_outputs, 1):\n",
    "    print(f\"Paraphrase {i}: {paraphrase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6d928f-d613-4b39-aa45-feae2faf4fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase 1: The restaurant is a carve-off space up two flights of stairs to one side, dominated by faux wood floors and brick columns.\n",
      "Paraphrase 2: The restaurant is a carve-off space up two flights of stairs to one side, dominated by faux brick columns and fake wood floors. There's an air foetid despondency about the place as you walk\n",
      "Paraphrase 3: The restaurant is a carve-off space up two flights of stairs to one side, dominated by faux brick columns and fake wood floors.\n",
      "Paraphrase 4: The restaurant is a carve-off space up two flights of stairs to one side, dominated by faux brick columns and fake wood floors. There's an air foetid despondency about the place as you enter\n",
      "Paraphrase 5: The restaurant is a carve-off space up two flights of stairs to one side, dominated by faux brick columns and fake wood floors. There's an air foetid despondency about the place as it sit\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_sentence = \"The restaurant is a carved-off space up a couple of stairs to one side, dominated by faux bare-brick columns, faux-wood floors and an air of foetid despondency\"\n",
    "paraphrase_outputs = generate_paraphrases(input_sentence, num_returns=5)\n",
    "for i, paraphrase in enumerate(paraphrase_outputs, 1):\n",
    "    print(f\"Paraphrase {i}: {paraphrase}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
