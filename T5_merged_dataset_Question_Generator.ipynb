{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load the dataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362399/362399 [56:17<00:00, 107.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into Train and Validation...\n",
      "Shuffling DataFrames...\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_squad_dataset(csv_file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    dataset = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Initialize an empty DataFrame with specified columns\n",
    "    df_dataset = pd.DataFrame(columns=['context', 'question', 'answer'])\n",
    "    num_of_answer = 0\n",
    "    \n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        answer = str(row['answer'])\n",
    "        number_of_words = len(answer.split())\n",
    "        df_dataset.loc[num_of_answer] = [context] + [question] + [answer]\n",
    "        num_of_answer += 1\n",
    "    \n",
    "    return df_dataset\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"Loading Dataset...\")\n",
    "df = load_squad_dataset(\"./merged_dataset.csv\")\n",
    "\n",
    "# Split the dataframe into train and validation\n",
    "print(\"Splitting into Train and Validation...\")\n",
    "train_size = int(0.95 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "df_train = df.iloc[:train_size]\n",
    "df_validation = df.iloc[train_size:]\n",
    "\n",
    "print('Shuffling DataFrames...')\n",
    "df_train = shuffle(df_train)\n",
    "df_validation = shuffle(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save the dataset as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset as csv...\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "print('Saving dataset as csv...')\n",
    "dataset_save_path = 'datasets/'\n",
    "if not os.path.exists(dataset_save_path):\n",
    "    os.makedirs(dataset_save_path)\n",
    "df_train.to_csv(dataset_save_path + 'squad_train.csv', index=False)\n",
    "df_validation.to_csv(dataset_save_path + 'squad_validation.csv', index=False)\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a QG-Algorithm, Model: T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Initial Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "train_file_path = 'datasets/squad_train.csv'\n",
    "validation_file_path = 'datasets/squad_validation.csv'\n",
    "save_model_path = 'model/'\n",
    "save_tokenizer_path = 'tokenizer/'\n",
    "pretrained_model = 't5-large'\n",
    "\n",
    "# Set training arguments\n",
    "args = {\n",
    "    'num_workers': 2,\n",
    "    'batch_size': 16,  # Original: 4\n",
    "    'learning_rate': 1e-4,  # Original: 3e-5\n",
    "    'eps': 1e-8,\n",
    "    'weight_decay': 0.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, max_len_input=512, max_len_output=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.max_len_input = max_len_input\n",
    "        self.max_len_output = max_len_output\n",
    "        self.context_column = 'context'\n",
    "        self.answer_column = 'answer'\n",
    "        self.question_column = 'question'\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self._load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index]['input_ids'].squeeze()\n",
    "        target_ids = self.targets[index]['input_ids'].squeeze()\n",
    "        source_mask = self.inputs[index]['attention_mask'].squeeze()\n",
    "        target_mask = self.targets[index]['attention_mask'].squeeze()\n",
    "        labels = copy.deepcopy(target_ids)\n",
    "        labels[labels == 0] = -100\n",
    "        return {'source_ids': source_ids, 'source_mask': source_mask, 'target_ids': target_ids, 'target_mask': target_mask, 'labels': labels}\n",
    "\n",
    "    def _load_data(self):\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            context, answer, target = self.data.loc[idx, self.context_column], self.data.loc[idx, self.answer_column], self.data.loc[idx, self.question_column]\n",
    "            input_text = '<answer> %s <context> %s ' % (answer, context)\n",
    "            target = str(target)\n",
    "\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_text],\n",
    "                max_length=self.max_len_input,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target],\n",
    "                max_length=self.max_len_output,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New (chrF loss included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.text import CHRFScore\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer, args, chrf_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = args\n",
    "        self.chrf_weight = chrf_weight\n",
    "        self.chrf_metric = CHRFScore()\n",
    "        self.save_hyperparameters(ignore=['model', 'tokenizer'])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Generate questions\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            max_length=128,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "        # Decode generated questions and ground truth\n",
    "        generated_questions = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        # true_questions = self.tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "        true_questions = self.tokenizer.batch_decode(batch['labels'][batch['labels'] != -100], skip_special_tokens=True)\n",
    "\n",
    "        max_length = max(len(generated_questions), len(true_questions))\n",
    "        generated_questions = generated_questions + [''] * (max_length - len(generated_questions))\n",
    "        true_questions = true_questions + [''] * (max_length - len(true_questions))\n",
    "        \n",
    "        # Calculate CHRF score\n",
    "        chrf_score = self.chrf_metric(generated_questions, true_questions)\n",
    "\n",
    "        # Combine losses\n",
    "        combined_loss = (1 - self.chrf_weight) * loss + self.chrf_weight * chrf_score\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log('train_chrf', chrf_score, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log('train_combined_loss', combined_loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        return combined_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Generate questions\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            max_length=128,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        \n",
    "        # Decode generated questions and ground truth\n",
    "        generated_questions = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        # true_questions = self.tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "        true_questions = self.tokenizer.batch_decode(batch['labels'][batch['labels'] != -100], skip_special_tokens=True)\n",
    "\n",
    "        max_length = max(len(generated_questions), len(true_questions))\n",
    "        generated_questions = generated_questions + [''] * (max_length - len(generated_questions))\n",
    "        true_questions = true_questions + [''] * (max_length - len(true_questions))\n",
    "        \n",
    "        # Calculate CHRF score\n",
    "        chrf_score = self.chrf_metric(generated_questions, true_questions)\n",
    "\n",
    "        # Combine losses\n",
    "        combined_loss = (1 - self.chrf_weight) * loss + self.chrf_weight * chrf_score\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log('val_chrf', chrf_score, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log('val_combined_loss', combined_loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        return combined_loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=self.args['batch_size'], num_workers=self.args['num_workers'])\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(validation_dataset, batch_size=self.args['batch_size'], num_workers=self.args['num_workers'])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.args['learning_rate'], eps=self.args['eps'])\n",
    "\n",
    "    def save_model(self, save_model_path):\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(save_model_path):\n",
    "            os.makedirs(save_model_path)\n",
    "        # Save the model\n",
    "        self.model.save_pretrained(save_model_path)\n",
    "        \n",
    "    def save_tokenizer(self, save_tokenizer_path):\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(save_tokenizer_path):\n",
    "            os.makedirs(save_tokenizer_path)\n",
    "        # Save the tokenizer\n",
    "        self.tokenizer.save_pretrained(save_tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer, args):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=self.args['batch_size'], num_workers=self.args['num_workers'])\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(validation_dataset, batch_size=self.args['batch_size'], num_workers=self.args['num_workers'])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.args['learning_rate'], eps=self.args['eps'])\n",
    "    \n",
    "    def save_model(self, save_model_path):\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(save_model_path):\n",
    "            os.makedirs(save_model_path)\n",
    "        # Save the model\n",
    "        self.model.save_pretrained(save_model_path)\n",
    "        \n",
    "    def save_tokenizer(self, save_tokenizer_path):\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(save_tokenizer_path):\n",
    "            os.makedirs(save_tokenizer_path)\n",
    "        # Save the tokenizer\n",
    "        self.tokenizer.save_pretrained(save_tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrhnylmz14\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cta/users/serhan.yilmaz/llama3-phi3/QuestionGeneration/T5-scaled/wandb/run-20240702_185840-2h5cetx4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srhnylmz14/t5-question-generation/runs/2h5cetx4' target=\"_blank\">training-run-pl-3</a></strong> to <a href='https://wandb.ai/srhnylmz14/t5-question-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srhnylmz14/t5-question-generation' target=\"_blank\">https://wandb.ai/srhnylmz14/t5-question-generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srhnylmz14/t5-question-generation/runs/2h5cetx4' target=\"_blank\">https://wandb.ai/srhnylmz14/t5-question-generation/runs/2h5cetx4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/srhnylmz14/t5-question-generation/runs/2h5cetx4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14dbd951ef80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"t5-question-generation\", name=\"training-run-pl-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading pre-trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pl.seed_everything(99)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "print('Loading pre-trained model...')\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_model, model_max_length=512)\n",
    "tokenizer.add_special_tokens(\n",
    "    {'additional_special_tokens': ['<answer>', '<context>']}\n",
    ")\n",
    "model = T5ForConditionalGeneration.from_pretrained(pretrained_model).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344279/344279 [15:21<00:00, 373.48it/s]\n",
      "100%|██████████| 18120/18120 [00:36<00:00, 491.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:  344279\n",
      "validation_dataset:  18120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Preparing dataset...')\n",
    "train_dataset = QGDataset(tokenizer, train_file_path)\n",
    "validation_dataset = QGDataset(tokenizer, validation_file_path)\n",
    "\n",
    "print('train_dataset: ', len(train_dataset))\n",
    "print('validation_dataset: ', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high') # original: highest - available: highest, medium, high\n",
    "# see more on that: https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /cta/users/serhan.yilmaz/.local/lib/python3.10/site- ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "print('Initializing model...')\n",
    "model = T5FineTuner(model, tokenizer, args, chrf_weight=0.2)\n",
    "# model = T5FineTuner(model, tokenizer, args)\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"t5-question-generation\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\")],\n",
    "    logger=wandb_logger  # Add this line\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type                       | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model       | T5ForConditionalGeneration | 737 M \n",
      "1 | chrf_metric | CHRFScore                  | 0     \n",
      "-----------------------------------------------------------\n",
      "737 M     Trainable params\n",
      "0         Non-trainable params\n",
      "737 M     Total params\n",
      "2,950.566 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/torchmetrics/functional/text/chrf.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  total_n_grams[n] = tensor(sum(n_grams_counts[n].values()))\n",
      "/cta/users/serhan.yilmaz/.conda/envs/unsloth_env/lib/python3.10/site-packages/torchmetrics/functional/text/chrf.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  matching_n_grams[n] = tensor(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1e7474911c4f1a8f4806a327d0608a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Fine tuning...')\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Total time: 44.03426079471906 hours\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "print('Saving model...')\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.makedirs(save_model_path)\n",
    "if not os.path.exists(save_tokenizer_path):\n",
    "    os.makedirs(save_tokenizer_path)\n",
    "model.model.save_pretrained(save_model_path)\n",
    "tokenizer.save_pretrained(save_tokenizer_path)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print('Total time: %s hours' % (end_time / 60 / 60))\n",
    "wandb.finish()\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "trained_model = 'all-distilroberta-v1'\n",
    "# trained_model = 'all-roberta-large-v1'\n",
    "class SentenceEmbeddings:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(trained_model)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.embedder.encode(text, convert_to_tensor=True)\n",
    "\n",
    "    def get_most_similar(self, context:str, qa_list:list):\n",
    "        context_embeddings = self.encode(context)\n",
    "        top1 = {'idx': 0, 'score': float('-inf')}\n",
    "        for i in range(len(qa_list)):\n",
    "            qa_str = qa_list[i]['question'] + ' ' + qa_list[i]['answer']\n",
    "            qa_embeddings = self.encode(qa_str)\n",
    "            cos_score = util.pytorch_cos_sim(context_embeddings, qa_embeddings)\n",
    "            # print(cos_score[0][0], qa_list[i])\n",
    "            if cos_score[0][0] > top1['score']:\n",
    "                top1['score'] = cos_score[0][0]\n",
    "                top1['idx'] = i\n",
    "        return qa_list[top1['idx']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QG-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "trained_model_path = save_model_path\n",
    "trained_tokenizer_path = save_tokenizer_path\n",
    "\n",
    "class QuestionGeneration:\n",
    "\n",
    "    def __init__(self, model_dir=None):\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer_path)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate(self, answer: str, context: str):\n",
    "        input_text = '<answer> %s <context> %s ' % (answer, context)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams = 3,\n",
    "            num_return_sequences = 1\n",
    "        )\n",
    "        question_list = []\n",
    "        for output in outputs:\n",
    "            question = self.tokenizer.decode(\n",
    "                output,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            question_list.append({'question': question, 'answer': answer, 'context': context})\n",
    "        return question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/cta/users/serhan.yilmaz/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who has fine-tuned T5 on SQuAD?', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'What is the name of the dataset that Serhan uses for question generation?', 'answer': 'SQuAD', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'What is T5 used for?', 'answer': 'question generation', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n"
     ]
    }
   ],
   "source": [
    "context = '''\n",
    "Serhan has fine-tuned T5 on SQuAD dataset for question generation.\n",
    "'''\n",
    "answer_list = ['Serhan', 'SQuAD', 'question generation']\n",
    "\n",
    "QG = QuestionGeneration()\n",
    "SE = SentenceEmbeddings()\n",
    "\n",
    "for answer in answer_list:\n",
    "    qa_pair_list = QG.generate(answer, context)\n",
    "    most_similar = SE.get_most_similar(context, qa_pair_list)\n",
    "    print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who has fine-tuned T5 on SQuAD?', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'What is the name of the dataset that Serhan uses for question generation?', 'answer': 'SQuAD', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'What is T5 used for?', 'answer': 'question generation', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'Who has fine-tuned T5 on SQuAD?', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n"
     ]
    }
   ],
   "source": [
    "context = '''\n",
    "Serhan has fine-tuned T5 on SQuAD dataset for question generation.\n",
    "'''\n",
    "answer_list = ['Serhan', 'SQuAD', 'question generation', 'Serhan']\n",
    "\n",
    "QG = QuestionGeneration()\n",
    "SE = SentenceEmbeddings()\n",
    "\n",
    "for answer in answer_list:\n",
    "    qa_pair_list = QG.generate(answer, context)\n",
    "    most_similar = SE.get_most_similar(context, qa_pair_list)\n",
    "    print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGeneration:\n",
    "    def __init__(self, model_dir=None):\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer_path)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate(self, answer: str, context: str, temperature: float = 2.5):\n",
    "        input_text = '<answer> %s <context> %s ' % (answer, context)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=3,\n",
    "            num_return_sequences=1,\n",
    "            temperature=temperature,  # Add temperature parameter\n",
    "            do_sample=True  # Enable sampling\n",
    "        )\n",
    "        question_list = []\n",
    "        for output in outputs:\n",
    "            question = self.tokenizer.decode(\n",
    "                output,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            question_list.append({'question': question, 'answer': answer, 'context': context})\n",
    "        return question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Which of the following people is not an entity: Serhan, the data, or question generation', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'What platform is T5 built on?', 'answer': 'SQuAD', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'What was T5 used for in the final step of development?', 'answer': 'question generation', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'Who fine-tuned T5 on SQuAD?', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n"
     ]
    }
   ],
   "source": [
    "context = '''\n",
    "Serhan has fine-tuned T5 on SQuAD dataset for question generation.\n",
    "'''\n",
    "answer_list = ['Serhan', 'SQuAD', 'question generation', 'Serhan']\n",
    "\n",
    "QG = QuestionGeneration()\n",
    "SE = SentenceEmbeddings()\n",
    "\n",
    "for answer in answer_list:\n",
    "    qa_pair_list = QG.generate(answer, context)\n",
    "    most_similar = SE.get_most_similar(context, qa_pair_list)\n",
    "    print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who has fine tuned the T5 on the SQuAD dataset?', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'What was the dataset used for question generation?', 'answer': 'SQuAD', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'what uses the data?', 'answer': 'question generation', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n",
      "{'question': 'Which organization has adjusted and updated T5 on SQuAD?', 'answer': 'Serhan', 'context': '\\nSerhan has fine-tuned T5 on SQuAD dataset for question generation.\\n'}\n"
     ]
    }
   ],
   "source": [
    "context = '''\n",
    "Serhan has fine-tuned T5 on SQuAD dataset for question generation.\n",
    "'''\n",
    "answer_list = ['Serhan', 'SQuAD', 'question generation', 'Serhan']\n",
    "\n",
    "QG = QuestionGeneration()\n",
    "SE = SentenceEmbeddings()\n",
    "\n",
    "for answer in answer_list:\n",
    "    qa_pair_list = QG.generate(answer, context)\n",
    "    most_similar = SE.get_most_similar(context, qa_pair_list)\n",
    "    print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
